{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2b86eb-9be5-49fd-90ff-54a2b0eee692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, DistilBertForSequenceClassification \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206c1211-fbb5-448f-ab4c-022f49e79270",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_academic_dataset_review.json\"\n",
    "WRITE_PATH = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_academic_dataset_review_short.json\"\n",
    "FINAL_WRITE_PATH = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_restaurant_reviews.parquet\"\n",
    "FINAL_WRITE_PATH_TRAIN = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_restaurant_reviews_train.parquet\"\n",
    "FINAL_WRITE_PATH_TEST = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_restaurant_reviews_test.parquet\"\n",
    "ALREADY_DATA = True\n",
    "ACTUALLY_TRAIN = True\n",
    "DISTIL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3d6ba1-5401-434d-8703-6559fe11a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALREADY_DATA:\n",
    "    with open(DATA_PATH, mode='r', encoding='utf8') as in_file, \\\n",
    "        open(WRITE_PATH, mode='w', encoding='utf8') as out_file:\n",
    "            for i in range(300000):\n",
    "                out_file.write(next(in_file))\n",
    "    df = pd.read_json(WRITE_PATH, lines=True)\n",
    "    print(df[\"stars\"].value_counts())\n",
    "    print(\"-\"*50)\n",
    "    print(df.count())\n",
    "\n",
    "    df['text_length'] = df['text'].apply(len)\n",
    "    sns.displot(df,x='text_length')\n",
    "    \n",
    "    df_train = df.loc[df['text_length'] <= 2500]\n",
    "    df_train[\"stars\"] -= 1\n",
    "    \n",
    "    sns.displot(df_train,x='text_length')\n",
    "    train, test = train_test_split(df_train, test_size=0.1)\n",
    "    train.to_parquet(FINAL_WRITE_PATH_TRAIN)\n",
    "    test.to_parquet(FINAL_WRITE_PATH_TEST)\n",
    "else:\n",
    "    df_train = pd.read_parquet(FINAL_WRITE_PATH_TRAIN)\n",
    "    df_test = pd.read_parquet(FINAL_WRITE_PATH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e3cdd2b0-ec53-45a1-9cc6-7eda15639189",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ACTUALLY_TRAIN:\n",
    "    df = df_train.sample(n=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40edbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.sample(n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de64a83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114941</th>\n",
       "      <td>41INyvULcU96e4cHfMXVhw</td>\n",
       "      <td>MIxQkyyvKg1ZgLvdihSigQ</td>\n",
       "      <td>PnJOVC9WGuMrNi5vQ04gMA</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Great atmosphere....professional, upscale and ...</td>\n",
       "      <td>2013-06-27 11:41:54</td>\n",
       "      <td>916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234394</th>\n",
       "      <td>k3dywgFifSpAaQGJ8rpk1g</td>\n",
       "      <td>5zNVELV8Huei5LEvAdUuXg</td>\n",
       "      <td>WbA5ud4InNWkizW7HE5kRQ</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Gluten-free paradise and good value.  I'm not ...</td>\n",
       "      <td>2014-12-13 20:38:42</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142658</th>\n",
       "      <td>RvDWy-1pcT84cPUMvkaZ1Q</td>\n",
       "      <td>ysV_DyxdGTyxngavPMCayA</td>\n",
       "      <td>hJrhf5N_B8ifYqHQJwFAEw</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The meats were pretty tasty, but the portions ...</td>\n",
       "      <td>2017-07-22 23:46:11</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254688</th>\n",
       "      <td>cqdw8L8cQVeSOU8zxKBoEQ</td>\n",
       "      <td>5ASyvGMmI-gwmfrlWKc2WQ</td>\n",
       "      <td>1CCaaunP7_hKs7wXSSJsKA</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This chain is one of my favorite chains to sta...</td>\n",
       "      <td>2019-04-12 13:10:12</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220527</th>\n",
       "      <td>35-0SuFXrRxIaJeQJM0_SA</td>\n",
       "      <td>txwFfvGkFEfqnaxVmFGOsw</td>\n",
       "      <td>0bPLkL0QhhPO5kt1_EXmNQ</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>When I was a kid, growing up in New York - the...</td>\n",
       "      <td>2009-12-03 00:38:28</td>\n",
       "      <td>855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82712</th>\n",
       "      <td>B8k0mQAkrx9arVMEDNKsvA</td>\n",
       "      <td>7WZ1YuYYW5Ez3yzzulA7Kw</td>\n",
       "      <td>S3QHy1sshUeZwXOYviVsXQ</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I know they have a good cappuccino:) waffles n...</td>\n",
       "      <td>2015-01-06 22:59:46</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47017</th>\n",
       "      <td>R8FzJEytGgkLzzTBWZ-JUQ</td>\n",
       "      <td>u5WMfKHWsVsYCIEatr6UAw</td>\n",
       "      <td>tr366vgAkbcpJBVKSdBxZg</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>They're reopened as of 10/27/16!!\\n\\nAmazing p...</td>\n",
       "      <td>2016-10-28 01:17:35</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157797</th>\n",
       "      <td>mp_YYfuBClBMYh0MhyF-lA</td>\n",
       "      <td>C0h0_bwIAUIuml-8n5rAvA</td>\n",
       "      <td>u7_3L1NBWgxhBM_B-cmmnA</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Awesome pizza...the crust is perfect. It's lig...</td>\n",
       "      <td>2014-05-30 20:48:21</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211528</th>\n",
       "      <td>mS2FkYCJKYrQB8i-7VMdcA</td>\n",
       "      <td>5smqsNwhlRFu32YJxMgCIw</td>\n",
       "      <td>q0Fi4n7shUTmlxl-mMPVXA</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I've never been to an urgent care but a sick d...</td>\n",
       "      <td>2015-12-30 01:20:07</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205543</th>\n",
       "      <td>rD1R-1pdCNQRyV7H_1YZtw</td>\n",
       "      <td>i6pB7aNYVNMczrqrDMqVKw</td>\n",
       "      <td>Ouz6NevgUHn-rGXbcw7DOQ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>STOP WRITING ON YOUR RECORD SLEEVES AND BOOKS!...</td>\n",
       "      <td>2011-02-27 14:19:08</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review_id                 user_id  \\\n",
       "114941  41INyvULcU96e4cHfMXVhw  MIxQkyyvKg1ZgLvdihSigQ   \n",
       "234394  k3dywgFifSpAaQGJ8rpk1g  5zNVELV8Huei5LEvAdUuXg   \n",
       "142658  RvDWy-1pcT84cPUMvkaZ1Q  ysV_DyxdGTyxngavPMCayA   \n",
       "254688  cqdw8L8cQVeSOU8zxKBoEQ  5ASyvGMmI-gwmfrlWKc2WQ   \n",
       "220527  35-0SuFXrRxIaJeQJM0_SA  txwFfvGkFEfqnaxVmFGOsw   \n",
       "...                        ...                     ...   \n",
       "82712   B8k0mQAkrx9arVMEDNKsvA  7WZ1YuYYW5Ez3yzzulA7Kw   \n",
       "47017   R8FzJEytGgkLzzTBWZ-JUQ  u5WMfKHWsVsYCIEatr6UAw   \n",
       "157797  mp_YYfuBClBMYh0MhyF-lA  C0h0_bwIAUIuml-8n5rAvA   \n",
       "211528  mS2FkYCJKYrQB8i-7VMdcA  5smqsNwhlRFu32YJxMgCIw   \n",
       "205543  rD1R-1pdCNQRyV7H_1YZtw  i6pB7aNYVNMczrqrDMqVKw   \n",
       "\n",
       "                   business_id  stars  useful  funny  cool  \\\n",
       "114941  PnJOVC9WGuMrNi5vQ04gMA      4       3      0     0   \n",
       "234394  WbA5ud4InNWkizW7HE5kRQ      3       0      0     0   \n",
       "142658  hJrhf5N_B8ifYqHQJwFAEw      1       1      0     0   \n",
       "254688  1CCaaunP7_hKs7wXSSJsKA      3       0      0     0   \n",
       "220527  0bPLkL0QhhPO5kt1_EXmNQ      3       3      0     0   \n",
       "...                        ...    ...     ...    ...   ...   \n",
       "82712   S3QHy1sshUeZwXOYviVsXQ      3       1      0     0   \n",
       "47017   tr366vgAkbcpJBVKSdBxZg      4       2      0     1   \n",
       "157797  u7_3L1NBWgxhBM_B-cmmnA      3       0      0     0   \n",
       "211528  q0Fi4n7shUTmlxl-mMPVXA      4       0      0     1   \n",
       "205543  Ouz6NevgUHn-rGXbcw7DOQ      1       1      2     0   \n",
       "\n",
       "                                                     text                date  \\\n",
       "114941  Great atmosphere....professional, upscale and ... 2013-06-27 11:41:54   \n",
       "234394  Gluten-free paradise and good value.  I'm not ... 2014-12-13 20:38:42   \n",
       "142658  The meats were pretty tasty, but the portions ... 2017-07-22 23:46:11   \n",
       "254688  This chain is one of my favorite chains to sta... 2019-04-12 13:10:12   \n",
       "220527  When I was a kid, growing up in New York - the... 2009-12-03 00:38:28   \n",
       "...                                                   ...                 ...   \n",
       "82712   I know they have a good cappuccino:) waffles n... 2015-01-06 22:59:46   \n",
       "47017   They're reopened as of 10/27/16!!\\n\\nAmazing p... 2016-10-28 01:17:35   \n",
       "157797  Awesome pizza...the crust is perfect. It's lig... 2014-05-30 20:48:21   \n",
       "211528  I've never been to an urgent care but a sick d... 2015-12-30 01:20:07   \n",
       "205543  STOP WRITING ON YOUR RECORD SLEEVES AND BOOKS!... 2011-02-27 14:19:08   \n",
       "\n",
       "        text_length  \n",
       "114941          916  \n",
       "234394          683  \n",
       "142658          276  \n",
       "254688          126  \n",
       "220527          855  \n",
       "...             ...  \n",
       "82712           187  \n",
       "47017           119  \n",
       "157797          204  \n",
       "211528          242  \n",
       "205543          396  \n",
       "\n",
       "[100000 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0eb45b3-9457-47fd-88d6-ecfdd0f4c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 100000/100000 [00:38<00:00, 2615.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(input_text, tokenizer):\n",
    "    return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        padding = \"max_length\",\n",
    "                        max_length=256,\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "# Initialize Bert Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True)\n",
    "\n",
    "text = df.text.values\n",
    "labels = df.stars.values\n",
    "\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "for sample in tqdm(text):\n",
    "    encoding_dict = preprocess(sample, tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids'])\n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = F.one_hot(torch.tensor(labels), num_classes=5).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80a0d960-1e1e-4003-a4d1-3a42a679d62b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DISTIL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1564\\4040019803.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the BertForSequenceClassification model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mDISTIL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     model = BertForSequenceClassification.from_pretrained(\n\u001b[0;32m      4\u001b[0m         \u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DISTIL' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the BertForSequenceClassification model\n",
    "if not DISTIL:\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels = 5,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # Load the DistilBertForSequenceClassification model\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels = 5,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25290024-476f-4b21-8329-c5beb3d133b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a0f728-28fc-4ff1-800b-370c6a0f39df",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ratio = 0.2\n",
    "batch_size = 16\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size = 0.2,\n",
    "    shuffle = True,\n",
    "    stratify = labels)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9bdb616-7a50-4f68-a194-02066797e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_accuracy(logits, labels):\n",
    "    preds = np.argmax(logits, axis = 1).flatten()\n",
    "    truth = np.argmax(labels, axis=1).flatten()\n",
    "    return accuracy_score(truth, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba7a3854-bef9-4cc5-9180-a36f7bcd7a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|█████████████████████████▎                                                  | 1/3 [15:11<30:23, 911.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.1984\n",
      "\t - Validation Accuracy: 0.6855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████████████████████████████████████████████████▋                         | 2/3 [30:22<15:10, 910.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.1559\n",
      "\t - Validation Accuracy: 0.6814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|████████████████████████████████████████████████████████████████████████████| 3/3 [45:32<00:00, 910.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.1198\n",
      "\t - Validation Accuracy: 0.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pytorch training loop\n",
    "\n",
    "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 2e-5,\n",
    "                              eps = 1e-08)\n",
    "\n",
    "# Run on GPU\n",
    "model.cuda()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "epochs = 3\n",
    "\n",
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    train_loss = 0\n",
    "    nb_train_examples, nb_train_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids,  \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update tracking variables\n",
    "        train_loss += train_output.loss.item()\n",
    "        nb_train_examples += b_input_ids.size(0)\n",
    "        nb_train_steps += 1\n",
    "\n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    eval_loss = []\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              attention_mask = b_input_mask)\n",
    "        eval_loss.append(eval_output.loss)\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate validation metrics\n",
    "        batch_accuracy = compute_batch_accuracy(logits, label_ids)\n",
    "        val_accuracy.append(batch_accuracy)\n",
    "    \n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(train_loss / nb_train_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c7afb",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "### Model 1\n",
    "50,000 Samples\n",
    "256 tokens\n",
    "3 epochs \n",
    "lr = 5e-5,\n",
    "eps = 1e-08\n",
    "Train loss: 0.2157\n",
    "Validation Accuracy: 0.6926\n",
    "\n",
    "### Model 2\n",
    "\n",
    "100,000 Samples\n",
    "256 tokens\n",
    "3 epochs \n",
    "lr = 5e-5,\n",
    "eps = 1e-08\n",
    "Train loss: 0.2251\n",
    "Validation Accuracy: 0.6970\n",
    "\n",
    "### Model 3\n",
    "\n",
    "100,000 Samples\n",
    "256 tokens\n",
    "2 epochs \n",
    "lr = 5e-5,\n",
    "eps = 1e-08\n",
    "Train loss: 0.2296\n",
    "Validation Accuracy: 0.7033\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e79523f9-1673-426a-a5ea-80b33a01edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of stars: 5\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"The waiters were really nice and great food.\", return_tensors=\"pt\")\n",
    "inputs.pop(\"token_type_ids\")\n",
    "model.cpu()\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "print(f\"The number of stars: {predicted_class_id + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3a56838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996, 15610,  2015,  2020,  2428,  3835,  1998,  2307,  2833,\n",
      "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68c1a0b9-a468-4481-9298-117974dbfb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"models/distil_50k_samples\")\n",
    "model.save_pretrained(\"models/distil_50k_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df658e0-2192-4382-82ef-3c0b517267d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class YelpRestaurantDataset(Dataset):\n",
    "#     def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_token_length: int = 512):\n",
    "#         self.data = data\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_token_length = max_token_length\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, index: int):\n",
    "#         row = self.data.iloc[index]\n",
    "#         input_text = row.text\n",
    "#         labels = row.stars\n",
    "#         encoding = tokenizer.encode_plus(\n",
    "#                             input_text,\n",
    "#                             add_special_tokens = True,\n",
    "#                             padding = \"max_length\",\n",
    "#                             max_length = self.max_token_length,\n",
    "#                             truncation = True,\n",
    "#                             return_attention_mask = True,\n",
    "#                             return_tensors = 'pt'\n",
    "#                        )\n",
    "#         print(labels)\n",
    "#         return dict(\n",
    "#             input_text = input_text,\n",
    "#             input_ids = encoding[\"input_ids\"].flatten(),\n",
    "#             attention_mask = encoding[\"attention_mask\"].flatten(),\n",
    "#             labels = torch.tensor(labels)\n",
    "#         )\n",
    "\n",
    "# dataset = YelpRestaurantDataset(df, tokenizer)\n",
    "\n",
    "# Hugging Face Training (Not used for now)\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=encoded_dataset[\"train\"],\n",
    "#     eval_dataset=encoded_dataset[validation_key],\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84d287",
   "metadata": {},
   "source": [
    "## TFIDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0361ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess_text(text, stem=False, lemmatize=True, remove_urls, stopwords=None):\n",
    "    if remove_urls:\n",
    "        text = remove_urls(text)\n",
    "    \n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize\n",
    "    text_list = word_tokenize()\n",
    "    ## remove Stopwords\n",
    "    if stopwords is not None:\n",
    "        text_list = [word for word in text_list if word not in stopwords]\n",
    "                \n",
    "    ## Stemming\n",
    "    if stem:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        text_list = [ps.stem(word) for word in text_list]\n",
    "                \n",
    "    ## Lemmatisation\n",
    "    if lemmatize:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        text_list = [lem.lemmatize(word) for word in text_list]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
