{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2b86eb-9be5-49fd-90ff-54a2b0eee692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, DistilBertForSequenceClassification \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "206c1211-fbb5-448f-ab4c-022f49e79270",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_academic_dataset_review.json\"\n",
    "WRITE_PATH = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_academic_dataset_review_short.json\"\n",
    "FINAL_WRITE_PATH = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_restaurant_reviews.parquet\"\n",
    "FINAL_WRITE_PATH_TRAIN = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_restaurant_reviews_train.parquet\"\n",
    "FINAL_WRITE_PATH_TEST = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_restaurant_reviews_test.parquet\"\n",
    "ALREADY_DATA = True\n",
    "ACTUALLY_TRAIN = True\n",
    "DISTIL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3d6ba1-5401-434d-8703-6559fe11a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALREADY_DATA:\n",
    "    with open(DATA_PATH, mode='r', encoding='utf8') as in_file, \\\n",
    "        open(WRITE_PATH, mode='w', encoding='utf8') as out_file:\n",
    "            for i in range(300000):\n",
    "                out_file.write(next(in_file))\n",
    "    df = pd.read_json(WRITE_PATH, lines=True)\n",
    "    print(df[\"stars\"].value_counts())\n",
    "    print(\"-\"*50)\n",
    "    print(df.count())\n",
    "\n",
    "    df['text_length'] = df['text'].apply(len)\n",
    "    sns.displot(df,x='text_length')\n",
    "    \n",
    "    df_train = df.loc[df['text_length'] <= 2500]\n",
    "    df_train[\"stars\"] -= 1\n",
    "    \n",
    "    sns.displot(df_train,x='text_length')\n",
    "    train, test = train_test_split(df_train, test_size=0.1)\n",
    "    train.to_parquet(FINAL_WRITE_PATH_TRAIN)\n",
    "    test.to_parquet(FINAL_WRITE_PATH_TEST)\n",
    "else:\n",
    "    df_train = pd.read_parquet(FINAL_WRITE_PATH_TRAIN)\n",
    "    df_test = pd.read_parquet(FINAL_WRITE_PATH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e3cdd2b0-ec53-45a1-9cc6-7eda15639189",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ACTUALLY_TRAIN:\n",
    "    df = df_train.sample(n=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40edbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.sample(n=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de64a83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31096</th>\n",
       "      <td>zbF_yFyW0KDdizoY7bwWXQ</td>\n",
       "      <td>GrlOQH1w3jIAhrziW6iuyQ</td>\n",
       "      <td>5ewePyN_mEE_79OmYyYBEA</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I work until midnight by the time I get to sah...</td>\n",
       "      <td>2014-12-02 16:33:29</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251347</th>\n",
       "      <td>1m-e2lA0wj6UdiH8yFa1nA</td>\n",
       "      <td>oKEcgplgzi4Nid6bexXY9Q</td>\n",
       "      <td>1Pxg1AMf0rEn9QF__ZYoWw</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>I am a spent ronin, balding, emaciated, with t...</td>\n",
       "      <td>2015-01-14 21:12:32</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74260</th>\n",
       "      <td>0Y48NTxXzEbQQsZDwZesQQ</td>\n",
       "      <td>UXbCcmkYGl3DH_Py5UOtbQ</td>\n",
       "      <td>UCMSWPqzXjd7QHq7v8PJjQ</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>We had lunch here today and loved it.  We were...</td>\n",
       "      <td>2015-01-22 05:21:20</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>8QTfi0GOXvK5Ig4-4q9Waw</td>\n",
       "      <td>nRBuwri2nzmFHUdOX9xO1Q</td>\n",
       "      <td>wQWhY5vA3ESMh6qFHMYvrg</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>My mani pedi at Queen Nails was great! I've be...</td>\n",
       "      <td>2014-03-17 15:42:54</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83771</th>\n",
       "      <td>FjRjpxUYZ-ntIQo-1Wa1Aw</td>\n",
       "      <td>bRCS17rhy_hif-X8Q8i_Dw</td>\n",
       "      <td>xkTjLbBC7uB-rAIDqAm-sw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Best tacos in Tampa I've tasted so far! I've b...</td>\n",
       "      <td>2015-06-09 21:41:28</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273519</th>\n",
       "      <td>7vemddPMpFDMHLeFCX6mlg</td>\n",
       "      <td>8pLZzJUhW_q13sKUKd-uZA</td>\n",
       "      <td>_O_KBH1MStcUkS1xk5pxsg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>- Really cute and quirky coffee shop that serv...</td>\n",
       "      <td>2016-09-26 01:04:22</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287570</th>\n",
       "      <td>qPXfjzFULcvYRbGWxPbqTg</td>\n",
       "      <td>pUNaC4U5JuY2TIoM6rsmmw</td>\n",
       "      <td>7apWV3_bxbRcC2MemII9dQ</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Excellent food, very reasonable prices, nice a...</td>\n",
       "      <td>2016-08-24 03:31:54</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31175</th>\n",
       "      <td>AF80XkMNM0jvPTKSxuzNKw</td>\n",
       "      <td>h9vwGgymTKanLx4DZj_ZaQ</td>\n",
       "      <td>kgeiJzWSiXPnf-3wx7LHIQ</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Welcome to Meridian, Panera! Love the Pick Two...</td>\n",
       "      <td>2016-02-01 15:53:53</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85668</th>\n",
       "      <td>WLEKpsGdq8eXhwqjNO2zhA</td>\n",
       "      <td>2tiS7fqBIwXMTHJe2GHUrg</td>\n",
       "      <td>mm4gSCCJXuAZFWDkESMjmw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I hate to do this.  My coworkers and I ate the...</td>\n",
       "      <td>2012-06-30 11:34:00</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172122</th>\n",
       "      <td>vJBB7pCpRiBu7ZJBNja7VA</td>\n",
       "      <td>5vpvlXT4NuzpiAAEmTYu2A</td>\n",
       "      <td>stJax2pMd3PVBbHZthlOdg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The management team at this location should pu...</td>\n",
       "      <td>2014-08-19 00:34:57</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267226 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review_id                 user_id  \\\n",
       "31096   zbF_yFyW0KDdizoY7bwWXQ  GrlOQH1w3jIAhrziW6iuyQ   \n",
       "251347  1m-e2lA0wj6UdiH8yFa1nA  oKEcgplgzi4Nid6bexXY9Q   \n",
       "74260   0Y48NTxXzEbQQsZDwZesQQ  UXbCcmkYGl3DH_Py5UOtbQ   \n",
       "3668    8QTfi0GOXvK5Ig4-4q9Waw  nRBuwri2nzmFHUdOX9xO1Q   \n",
       "83771   FjRjpxUYZ-ntIQo-1Wa1Aw  bRCS17rhy_hif-X8Q8i_Dw   \n",
       "...                        ...                     ...   \n",
       "273519  7vemddPMpFDMHLeFCX6mlg  8pLZzJUhW_q13sKUKd-uZA   \n",
       "287570  qPXfjzFULcvYRbGWxPbqTg  pUNaC4U5JuY2TIoM6rsmmw   \n",
       "31175   AF80XkMNM0jvPTKSxuzNKw  h9vwGgymTKanLx4DZj_ZaQ   \n",
       "85668   WLEKpsGdq8eXhwqjNO2zhA  2tiS7fqBIwXMTHJe2GHUrg   \n",
       "172122  vJBB7pCpRiBu7ZJBNja7VA  5vpvlXT4NuzpiAAEmTYu2A   \n",
       "\n",
       "                   business_id  stars  useful  funny  cool  \\\n",
       "31096   5ewePyN_mEE_79OmYyYBEA      0       4      1     0   \n",
       "251347  1Pxg1AMf0rEn9QF__ZYoWw      4       6     19     4   \n",
       "74260   UCMSWPqzXjd7QHq7v8PJjQ      4       8      6    10   \n",
       "3668    wQWhY5vA3ESMh6qFHMYvrg      4       0      0     0   \n",
       "83771   xkTjLbBC7uB-rAIDqAm-sw      4       1      0     0   \n",
       "...                        ...    ...     ...    ...   ...   \n",
       "273519  _O_KBH1MStcUkS1xk5pxsg      3       2      0     1   \n",
       "287570  7apWV3_bxbRcC2MemII9dQ      4       0      0     0   \n",
       "31175   kgeiJzWSiXPnf-3wx7LHIQ      3       2      0     0   \n",
       "85668   mm4gSCCJXuAZFWDkESMjmw      0       0      0     0   \n",
       "172122  stJax2pMd3PVBbHZthlOdg      0       1      0     0   \n",
       "\n",
       "                                                     text                date  \\\n",
       "31096   I work until midnight by the time I get to sah... 2014-12-02 16:33:29   \n",
       "251347  I am a spent ronin, balding, emaciated, with t... 2015-01-14 21:12:32   \n",
       "74260   We had lunch here today and loved it.  We were... 2015-01-22 05:21:20   \n",
       "3668    My mani pedi at Queen Nails was great! I've be... 2014-03-17 15:42:54   \n",
       "83771   Best tacos in Tampa I've tasted so far! I've b... 2015-06-09 21:41:28   \n",
       "...                                                   ...                 ...   \n",
       "273519  - Really cute and quirky coffee shop that serv... 2016-09-26 01:04:22   \n",
       "287570  Excellent food, very reasonable prices, nice a... 2016-08-24 03:31:54   \n",
       "31175   Welcome to Meridian, Panera! Love the Pick Two... 2016-02-01 15:53:53   \n",
       "85668   I hate to do this.  My coworkers and I ate the... 2012-06-30 11:34:00   \n",
       "172122  The management team at this location should pu... 2014-08-19 00:34:57   \n",
       "\n",
       "        text_length  \n",
       "31096           529  \n",
       "251347         1979  \n",
       "74260          2007  \n",
       "3668            199  \n",
       "83771           412  \n",
       "...             ...  \n",
       "273519          359  \n",
       "287570           95  \n",
       "31175           827  \n",
       "85668           254  \n",
       "172122          448  \n",
       "\n",
       "[267226 rows x 10 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0eb45b3-9457-47fd-88d6-ecfdd0f4c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200000/200000 [01:30<00:00, 2202.29it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(input_text, tokenizer):\n",
    "    return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        padding = \"max_length\",\n",
    "                        max_length=512,\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "# Initialize Bert Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True)\n",
    "\n",
    "text = df.text.values\n",
    "labels = df.stars.values\n",
    "\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "for sample in tqdm(text):\n",
    "    encoding_dict = preprocess(sample, tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids'])\n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = F.one_hot(torch.tensor(labels), num_classes=5).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80a0d960-1e1e-4003-a4d1-3a42a679d62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BertForSequenceClassification model\n",
    "if not DISTIL:\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels = 5,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # Load the DistilBertForSequenceClassification model\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels = 5,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25290024-476f-4b21-8329-c5beb3d133b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32a0f728-28fc-4ff1-800b-370c6a0f39df",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ratio = 0.2\n",
    "batch_size = 16\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size = validation_ratio,\n",
    "    shuffle = True,\n",
    "    stratify = labels)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9bdb616-7a50-4f68-a194-02066797e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_accuracy(logits, labels):\n",
    "    preds = np.argmax(logits, axis = 1).flatten()\n",
    "    truth = np.argmax(labels, axis=1).flatten()\n",
    "    return accuracy_score(truth, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba7a3854-bef9-4cc5-9180-a36f7bcd7a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                     | 1/4 [1:24:57<4:14:51, 5097.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.2618\n",
      "\t - Validation Accuracy: 0.7116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 2/4 [2:35:07<2:32:30, 4575.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.2230\n",
      "\t - Validation Accuracy: 0.7222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 3/4 [3:43:44<1:12:46, 4366.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.1942\n",
      "\t - Validation Accuracy: 0.7168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [4:52:21<00:00, 4385.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.1616\n",
      "\t - Validation Accuracy: 0.7117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pytorch training loop\n",
    "\n",
    "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 2e-5,\n",
    "                              eps = 1e-08)\n",
    "\n",
    "# Run on GPU\n",
    "model.cuda()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "epochs = 4\n",
    "\n",
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    train_loss = 0\n",
    "    nb_train_examples, nb_train_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids,  \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update tracking variables\n",
    "        train_loss += train_output.loss.item()\n",
    "        nb_train_examples += b_input_ids.size(0)\n",
    "        nb_train_steps += 1\n",
    "\n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    eval_loss = []\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              attention_mask = b_input_mask)\n",
    "        eval_loss.append(eval_output.loss)\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate validation metrics\n",
    "        batch_accuracy = compute_batch_accuracy(logits, label_ids)\n",
    "        val_accuracy.append(batch_accuracy)\n",
    "    \n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(train_loss / nb_train_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c7afb",
   "metadata": {},
   "source": [
    "## Hyperparameter Search\n",
    "\n",
    "### Model 1\n",
    "- 50,000 Samples\n",
    "- 256 tokens\n",
    "- 3 epochs \n",
    "- lr = 5e-5,\n",
    "- eps = 1e-08\n",
    "- Train loss: 0.2157\n",
    "- Validation Accuracy: 0.6926\n",
    "\n",
    "### Model 2\n",
    "\n",
    "- 100,000 Samples\n",
    "- 256 tokens\n",
    "- 3 epochs \n",
    "- lr = 5e-5,\n",
    "- eps = 1e-08\n",
    "- Train loss: 0.2251\n",
    "- Validation Accuracy: 0.6970\n",
    "\n",
    "### Model 3\n",
    "\n",
    "- 100,000 Samples\n",
    "- 256 tokens\n",
    "- 2 epochs \n",
    "- lr = 2e-5,\n",
    "- eps = 1e-08\n",
    "- Train loss: 0.2296\n",
    "- Validation Accuracy: 0.7033\n",
    "\n",
    "### Model 4\n",
    "- DistilBERT\n",
    "- 50,000 Samples\n",
    "- 256 tokens\n",
    "- 3 epochs \n",
    "- lr = 2e-5,\n",
    "- eps = 1e-08\n",
    "- Train loss: 0.1198\n",
    "- Validation Accuracy: 0.6770\n",
    "\n",
    "## Model 5\n",
    "- DistilBERT\n",
    "- 100,000 samples\n",
    "- 256 tokens\n",
    "- 3 epochs \n",
    "- lr = 2e-5,\n",
    "- eps = 1e-08\n",
    "- Train loss: 0.1949\n",
    "- Validation Accuracy: 0.6957\n",
    "\n",
    "## Model 6\n",
    "- DistilBERT\n",
    "- 100,000 samples\n",
    "- 512 tokens\n",
    "- 3 epochs \n",
    "- lr = 2e-5,\n",
    "- eps = 1e-08\n",
    "- Train loss: 0.1936\n",
    "- Validation Accuracy: 0.7076\n",
    "\n",
    "## Model 7\n",
    "- DistilBERT\n",
    "- 200,000 samples\n",
    "- 512 tokens\n",
    "- 4 epochs \n",
    "- lr = 2e-5,\n",
    "- eps = 1e-08\n",
    "- Train loss: 0.1616\n",
    "- Validation Accuracy: 0.7117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e79523f9-1673-426a-a5ea-80b33a01edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of stars: 5\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"The waiters were really nice and great food.\", return_tensors=\"pt\")\n",
    "inputs.pop(\"token_type_ids\")\n",
    "model.cpu()\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "print(f\"The number of stars: {predicted_class_id + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e95377c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996, 15610,  2015,  2020,  2428,  3835,  1998,  2307,  2833,\n",
      "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68c1a0b9-a468-4481-9298-117974dbfb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"models/distil_200k_samples_512_tokens\")\n",
    "model.save_pretrained(\"models/distil_200k_samples_512_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df658e0-2192-4382-82ef-3c0b517267d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class YelpRestaurantDataset(Dataset):\n",
    "#     def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_token_length: int = 512):\n",
    "#         self.data = data\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_token_length = max_token_length\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, index: int):\n",
    "#         row = self.data.iloc[index]\n",
    "#         input_text = row.text\n",
    "#         labels = row.stars\n",
    "#         encoding = tokenizer.encode_plus(\n",
    "#                             input_text,\n",
    "#                             add_special_tokens = True,\n",
    "#                             padding = \"max_length\",\n",
    "#                             max_length = self.max_token_length,\n",
    "#                             truncation = True,\n",
    "#                             return_attention_mask = True,\n",
    "#                             return_tensors = 'pt'\n",
    "#                        )\n",
    "#         print(labels)\n",
    "#         return dict(\n",
    "#             input_text = input_text,\n",
    "#             input_ids = encoding[\"input_ids\"].flatten(),\n",
    "#             attention_mask = encoding[\"attention_mask\"].flatten(),\n",
    "#             labels = torch.tensor(labels)\n",
    "#         )\n",
    "\n",
    "# dataset = YelpRestaurantDataset(df, tokenizer)\n",
    "\n",
    "# Hugging Face Training (Not used for now)\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=encoded_dataset[\"train\"],\n",
    "#     eval_dataset=encoded_dataset[validation_key],\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31954381",
   "metadata": {},
   "source": [
    "## TFIDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f4746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess_text(text, stem=False, lemmatize=True, remove_urls, stopwords=None):\n",
    "    if remove_urls:\n",
    "        text = remove_urls(text)\n",
    "    \n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize\n",
    "    text_list = word_tokenize()\n",
    "    ## remove Stopwords\n",
    "    if stopwords is not None:\n",
    "        text_list = [word for word in text_list if word not in stopwords]\n",
    "                \n",
    "    ## Stemming\n",
    "    if stem:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        text_list = [ps.stem(word) for word in text_list]\n",
    "                \n",
    "    ## Lemmatisation\n",
    "    if lemmatize:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        text_list = [lem.lemmatize(word) for word in text_list]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
