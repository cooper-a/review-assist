{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e2b86eb-9be5-49fd-90ff-54a2b0eee692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206c1211-fbb5-448f-ab4c-022f49e79270",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_academic_dataset_review.json\"\n",
    "WRITE_PATH = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_academic_dataset_review_short.json\"\n",
    "FINAL_WRITE_PATH = r\"C:\\Users\\zoopy\\code\\review-assist\\data\\yelp_restaurant_reviews.parquet\"\n",
    "ALREADY_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3d6ba1-5401-434d-8703-6559fe11a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALREADY_DATA:\n",
    "    with open(DATA_PATH, mode='r', encoding='utf8') as in_file, \\\n",
    "        open(WRITE_PATH, mode='w', encoding='utf8') as out_file:\n",
    "            for i in range(250000):\n",
    "                out_file.write(next(in_file))\n",
    "    df = pd.read_json(WRITE_PATH, lines=True)\n",
    "    print(df[\"stars\"].value_counts())\n",
    "    print(\"-\"*50)\n",
    "    print(df.count())\n",
    "\n",
    "    df['text_length'] = df['text'].apply(len)\n",
    "    sns.displot(df,x='text_length')\n",
    "\n",
    "    df = df.loc[df['text_length'] <= 2500]\n",
    "\n",
    "    sns.displot(df,x='text_length')\n",
    "    df.to_parquet(FINAL_WRITE_PATH)\n",
    "else:\n",
    "    df = pd.read_parquet(FINAL_WRITE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0eb45b3-9457-47fd-88d6-ecfdd0f4c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_text, tokenizer):\n",
    "  return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        padding = True,\n",
    "                        max_length=512,\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae95718-ee8d-4184-9c48-79af7cbf06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text.values\n",
    "labels = df.stars.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04883c99-31dc-45af-8a26-2c559ee53f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf098f8-2e54-4304-bf9f-8284c0a0b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 247444/247444 [06:05<00:00, 677.27it/s]\n"
     ]
    }
   ],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "for sample in tqdm(text):\n",
    "  encoding_dict = preprocess(sample, tokenizer)\n",
    "  token_id.append(encoding_dict['input_ids']) \n",
    "  attention_masks.append(encoding_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "decd3f8a-0227-4e85-bd26-8f1c811b0584",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 118 but got size 186 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21088\\1829535386.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoken_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mattention_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 118 but got size 186 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fc369f8-8891-413c-a831-3e1bd29467bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used for now\n",
    "# class YelpRestaurantDataset(Dataset):\n",
    "#     def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_token_length: int = 512):\n",
    "#         self.data = data\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_token_length = max_token_length\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, index: int):\n",
    "#         row = self.data.iloc[index]\n",
    "#         input_text = row.text\n",
    "#         labels = row.stars\n",
    "#         encoding = tokenizer.encode_plus(\n",
    "#                             input_text,\n",
    "#                             add_special_tokens = True,\n",
    "#                             padding = \"max_length\",\n",
    "#                             max_length = self.max_token_length,\n",
    "#                             truncation = True,\n",
    "#                             return_attention_mask = True,\n",
    "#                             return_tensors = 'pt'\n",
    "#                        )\n",
    "#         print(labels)\n",
    "#         return dict(\n",
    "#             input_text = input_text,\n",
    "#             input_ids = encoding[\"input_ids\"].flatten(),\n",
    "#             attention_mask = encoding[\"attention_mask\"].flatten(),\n",
    "#             labels = torch.tensor(labels)\n",
    "#         )\n",
    "\n",
    "# dataset = YelpRestaurantDataset(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80a0d960-1e1e-4003-a4d1-3a42a679d62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BertForSequenceClassification model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 5,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25290024-476f-4b21-8329-c5beb3d133b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_dataset(dataset, val_split=0.25):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['val'] = Subset(dataset, val_idx)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e448757d-9493-46f6-a22e-30ab97429bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ac0d31a-7423-4216-af1f-fd858b2d7cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 94649, 225431,   9729, ..., 206943,  86079, 159025])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32a0f728-28fc-4ff1-800b-370c6a0f39df",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21088\\2039662410.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Train and validation sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m train_set = TensorDataset(token_id[train_idx], \n\u001b[0m\u001b[0;32m     12\u001b[0m                           \u001b[0mattention_masks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                           labels[train_idx])\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "validation_ratio = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size = 0.2,\n",
    "    shuffle = True,\n",
    "    stratify = labels)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598c181-e84d-4e6a-a739-fd23a6f510dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a3854-bef9-4cc5-9180-a36f7bcd7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
